import os
if os.path.exists(r"C:\Users\Vidita Shetty\data")==False:
    os.mkdir(r"C:\Users\Vidita Shetty\data")
if os.path.exists(r"C:\Users\Vidita Shetty\data\training")==False:
    os.mkdir(r"C:\Users\Vidita Shetty\data\training")
if os.path.exists(r"C:\Users\Vidita Shetty\data\testing")==False:
    os.mkdir(r"C:\Users\Vidita Shetty\data\testing")
import cv2
vidobj=cv2.VideoCapture(0)
letter=65
present=True
count=1  
while present==True:
    present,frame=vidobj.read()
    frame=cv2.flip(frame,1)
    # Displaying the resulting frame
    cv2.imshow('frame', frame)
    if (cv2.waitKey(1) & 0xFF==ord(chr(letter)) ): 
        # stop producing frames and store in letter directory on clicking letter A,B...
        present,frame=vidobj.read()
        frame=cv2.flip(frame,1)
        
        # we want region of interset
        roi=cv2.rectangle(frame,(220-1,9),(620+1,409),(255,0,0),1)
        cv2.imshow('roi',roi)
        # Extracting the ROI
        froi = frame[10:410, 220:510]
        
        # now preprocessing it to only get a white outline of hand without skincolor,fingerprints,etc...
        froi=cv2.cvtColor(froi,cv2.COLOR_BGR2GRAY)
        froi = cv2.blur(froi,(5,5))
        froi = cv2.adaptiveThreshold(froi,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)
        # Displaying the resulting frame
        cv2.imshow('frame', froi)
        if os.path.exists(r"C:\Users\Vidita Shetty\data\training\\"+str(chr(letter)) ) ==False:
            os.mkdir(r"C:\Users\Vidita Shetty\data\training\\"+str(chr(letter)))
        cv2.imwrite((r"C:\Users\Vidita Shetty\data\training\\"+str(chr(letter))+"\\"+str(count)+".jpg"),froi)
        count+=1
    if (cv2.waitKey(1) & 0xFF==ord('q')) : # go to next letter on pressing n
        letter+=1
        print(chr(letter))
    if (cv2.waitKey(1) & 0xFF==ord('n')) : # quit on q
        break
vidobj.release()
cv2.destroyAllWindows()  
from keras import Sequential
from keras.layers import Conv2D,Dense,Flatten,MaxPooling2D
model=Sequential()
model.add(Conv2D(filters=16,kernel_size=2, input_shape=(128, 128, 1),padding="same",activation="relu"))
model.add(MaxPooling2D(pool_size=2))
model.add(Conv2D(filters=32,kernel_size=2, input_shape=(128, 128, 1),padding="same",activation ="relu"))
model.add(MaxPooling2D(pool_size=2))
model.add(Conv2D(filters=64,kernel_size=2, input_shape=(128, 128, 1),padding="same",activation="relu"))
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten()) ##will o/p the input to cnn
##input to cnn
model.add(Dense(500,activation="relu"))
model.add(Dense(500,activation="relu"))
model.add(Dense(26,activation="softmax"))
## compling
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
from keras.preprocessing.image import ImageDataGenerator
# Augmentation=> increase number of images and improves robustness of model by adding images with shifts horizontally,etc...
train_datagen = ImageDataGenerator( rescale=1./255,#rotation_range=5,  # rotation
                                   width_shift_range=0.2,  # horizontal shift
                                   zoom_range=0.2 ) # zoom

epochs = 25  # Epochs
batch_size = 32  # Batch size
train_gen=train_datagen.flow_from_directory(r"C:\Users\Vidita Shetty\data\training\\",
                                    target_size=(128,128), class_mode="categorical",color_mode='grayscale',
                                    batch_size=batch_size,interpolation="lanczos",shuffle=True)


test_datagen = ImageDataGenerator( rescale=1./255)
test_gen=train_datagen.flow_from_directory(r"C:\Users\Vidita Shetty\data\testing\\",
                                    target_size=(128,128), class_mode="categorical",color_mode='grayscale',
                                    batch_size=batch_size,interpolation="lanczos"#,shuffle=True #CHANGED THIS##########
                                          )

history =model.fit(
    x=train_gen, y=None, epochs=25 ,#48
    validation_data=test_gen, 
    verbose=2 , callbacks=None,
    shuffle=True, steps_per_epoch=20
)

history =model

# obtaining predictions manually 
import numpy
x, y_true= test_gen.next()
y_predict=model.predict(test_gen)
print("shape=",y_predict.shape)
print("shape of actual y=",y_true.shape)
## y predict for each of 26 classes has around 116 images of dim 28x28x1=>(26*116,26)(28,28,1)
#The NumPy module amax() function returns the maximum value of the numpy array or maximum value along an axis.
pred_classs=numpy.where(y_predict == numpy.amax(y_predict[17],axis=0)) #0.996 #index of pred_class_value
print("y_predict array for image=",y_predict[17])
print("max=",numpy.amax(y_predict[17]))
print("index of max prediction=",pred_classs)
a=pred_classs
# print(chr(a))
print(a[1])
b=a[1]
print(b[0])
print("ascii value of corresp. index =",chr(66+b[0])) # converts index to ascii character


print("Acual array/actual y=",y_true[17])
true_classs=numpy.where(y_true[17] ==1. )
print(true_classs)
c=true_classs[0]
print(c)
d=c[0]
print(d)
print("ACTUAL ascii value of corresp. index =",chr(66+d)) # converts index to ascii character
# ***
In the RGB color space, the light intensity of each channel is equal to the value of each channel. 
For example, if the value of each channel is set to 255, the color of the visualized color space results in white.
Also, if the value of each channel is set to 0, the color of the visualized color space results in black.
Equal values of each channel are represented by the same color in the RGB color space, but the combination 
of different values of each channel leads to different colors. The RGB color space is used to calculate the color of each pixel that can be displayed on the screen.
An RGBA color is composed of four channels, where each channel is represented by 8 bits. 
In RGBA, the transparency information is stored in the alpha channel, and the color information is 
stored in the three RGB channels.The alpha channel can be displayed as a grayscale image
from PIL import Image, ImageTk
import tkinter as tk
import cv2
import os
import numpy as np
import operator
import sys, os
import matplotlib.pyplot as plt
from string import ascii_uppercase

class Application:
    loaded_model=history
    def __init__(self):
        #initialize video capture object to capture 0/live webcam
        self.directory = 'model'
        self.vs = cv2.VideoCapture(0)
        #self.vs.release()
        self.current_image = None
        self.current_image2 = None
        
        #initialize the array ct containing labels/prediction i.e.blank,A,B,C...Z
        self.ct = {}
        self.ct['blank'] = 0
        for i in ascii_uppercase:
            self.ct[i] = 0
        print("Loaded model from disk")
        #initialize tkinter window called root
        self.root = tk.Tk()
        self.root.title("ASL Project")
        self.root.protocol('WM_DELETE_WINDOW', self.destructor)
        self.root.geometry("1100x1100")
        #place the live webcam on panel(image rectangle) 135-775 on x
        self.panel = tk.Label(self.root)
        self.panel.place(x = 135, y = 10, width = 640, height = 640)
        self.panel2 = tk.Label(self.root) # initialize image panel where hand is to be placed 460-770 on x
        self.panel2.place(x = 460, y = 95, width = 310, height = 310)
        
        ##initializing other labels like title,word,char,sentence as panel3,4,5
        self.T = tk.Label(self.root)
        self.T.place(x=31,y = 17)
        self.T.config(text = "ASL",font=("courier",40,"bold"))
        self.panel3 = tk.Label(self.root) # Current SYmbol
        self.panel3.place(x = 500,y=640)
        self.T1 = tk.Label(self.root)
        self.T1.place(x = 10,y = 640)
        self.T1.config(text="Character :",font=("Courier",40,"bold"))
        self.panel5 = tk.Label(self.root) # Sentence
        self.panel5.place(x = 350,y=520)##changed from 760
        self.T3 = tk.Label(self.root)
        self.T3.place(x = 10,y = 520)##changed from 760
        self.T3.config(text ="Sentence :",font=("Courier",40,"bold"))
        self.str=""
        self.word=""
        self.current_symbol="Empty"
        self.photo="Empty"
        self.video_loop()

    def video_loop(self):
        #ok returns true till frame captured/video is stopped
        ok, frame = self.vs.read()
        
        if ok:
            # to make a visible rectangle encapsulating hand in live webcam =>cv2.rectangle()
            # process the captured frame/hand by 1.flipping it around 2.converting to RGBA 3.get array 
            # 4.get only image of hand from x1,y1 to x2,y2 5.conv to grayscale 6.blurring hand lines& threshaold
            
            cv2image = cv2.flip(frame, 1) #480 x 640 frame by frame.shape[0 or 1]
            x1 = int(0.5*frame.shape[1])
            y1 = 10
            x2 = frame.shape[1]-10
            y2 = int(0.5*frame.shape[1])
            cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)
            cv2image = cv2.cvtColor(cv2image, cv2.COLOR_BGR2RGBA) #***
            self.current_image = Image.fromarray(cv2image)
            ##################### 
            img=Image.open("C:\\Users\\Vidita Shetty\\asl.png")
            imgg=ImageTk.PhotoImage(img,master=self.root)
            # Create a Label Widget to display the text or Image
            label1 = tk.Label(self.root, image=imgg)
            label1.place(x=100,y=700)
            #####################
            cv2image = cv2image[y1:y2, x1:x2]
            gray = cv2.cvtColor(cv2image, cv2.COLOR_BGR2GRAY)
            #blur = cv2.GaussianBlur(gray,(5,5),2) ## removed this
            blur = cv2.blur(gray,(5,5)) ## added this
            res = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)  ##added this
           #th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2) ## removed
           #ret, res = cv2.threshold(th3, 70, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU) ## removed this
        
           #displaying the processed image iinplace of unprocessed hand by config panel2 (label where hand is)
            self.predict(res) #calling the below predict method from current method
            self.current_image2 = Image.fromarray(res)
            imgtk = ImageTk.PhotoImage(image=self.current_image2)
          # after returning from predict method we set panels 2,3,4,5
            self.panel2.imgtk = imgtk
            self.panel2.config(image=imgtk)
            self.panel3.config(text=self.current_symbol,font=("Courier",50))
            self.panel5.config(text=self.str,font=("Courier",50))            
            self.root.after(30, self.video_loop)
    def predict(self,test_image):
         # predicting class after processing image to match training images
        test_image = cv2.resize(test_image, (128,128))
        result = self.loaded_model.predict(test_image.reshape(1, 128, 128, 1))
        # .predict() returns probability/scores of each class => r     blank    A    B   C    D    E ... Z
        #                                                       scores  0.1    0.2  0.2  0.2  0.1  0.0 ...0.0
        prediction={} # makes a DICTIONARY prediction with key=blank/A/B/... and value=probability of class
        prediction['blank'] = result[0][0] 
        inde = 0
        for i in ascii_uppercase:
            prediction[i] = result[0][inde]
            inde += 1
        #1. setting character
        prediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True) #sort the probabs in desc order(reverse=True),thus the class with highest probability at index 0
        self.current_symbol = prediction[0][0] #set class with highest probabli. as current character (key)
        #2. setting word,sentence
        if(self.current_symbol == 'blank'):
            for i in ascii_uppercase:
                self.ct[i] = 0
        #count correspondin gto symbol/character is inc. by 1 
        self.ct[self.current_symbol] += 1 
        #if this count exceeds 60 OR 60 continious/discontinious FRAMES/predictions of SAME letter is made
        if(self.ct[self.current_symbol] > 30): 
            # if count is signinficant => setting counts to 0 again for next char prediction
            self.ct['blank'] = 0
            # if current char is blank then put space in sentence and set word to "" to enable entering new word
            # else if current char is btw A-Z then add it to word
            if self.current_symbol == 'blank':
                    if len(self.str) > 0:
                        self.str += "-"
            else:
                self.str += self.current_symbol
           # if count is signinficant => setting counts to 0 again for next char prediction
            for i in ascii_uppercase:
                self.ct[i] = 0

    def destructor(self):
        print("Closing Application...")
        self.root.destroy()
        self.vs.release()
        cv2.destroyAllWindows()
       
print("Start")
pba = Application()
pba.root.mainloop()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
# obtaining predictions manually 
import numpy
import pandas as pd
#yTrue[[]]
yPred= [[0 for i in range(32)] for j in range (26)] 
yTrue= [[0 for i in range(32)] for j in range (26)] 
yConf= [[0 for i in range(26)] for j in range (26)] 
j=0
#while y_true.any()!=None:
for j in range(26): 
    for k in range(3):# so that a batch of 115 images is processed per letter 93/32=3.57 =3 so per class 32*3=96 IMAGES processed
        x, y_true= test_gen.next() #gives naxt batch with batch size defined above as 32 . so since each of 26 directories has 115 images shuffled so we pass range as 26*(115/32)=93.4=93
       # print("shape of image of 1 batch=",x.shape,"shape of label of 1 batch=",y_true.shape)
        y_predict=model.predict(x) #3013,26 is shape since each of 26 classes has about 115 images #test_gen.next()
       # print("SHAPE=",(y_predict.shape)) #32,36
        for i in range((y_predict.shape)[0]): #0-3012
            pred_classs=numpy.where(y_predict == numpy.amax(y_predict[i],axis=0)) #0.996 #index of pred_class_value
            true_classs=numpy.where(y_true[i] ==1. )
            a=pred_classs
            b=a[1]
           # print("ascii value of corresp. index =",chr(65+b[0])) # converts index to ascii character
            yPred[j][i]=chr(65+b[0])
            c=true_classs[0]
           # print(c[0])
           # print("ACTUAL ascii value of corresp. index =",chr(65+c[0])) 
            yTrue[j][i]=chr(65+c[0])
           # print("j=",j,"i=",i)
           # if yTrue[j][i]==yPred[j][i]: #only when we want to know images correctly classifed
            yConf[b[0]][c[0]]+=1
            k+=1
    j+=1
#print(yConf)

#Plotting the confusion matrix
plt.figure(figsize=(20,20))
yConf_df = pd.DataFrame(yConf,
                     index =['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 
                             'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 
                             'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], 
                     columns = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 
                                'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 
                                'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])
sns.heatmap(yConf_df,annot=True)
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

